# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import 'bangla.grm' as bn;
import 'classify.grm' as classify;
import 'universal_rules.grm' as universal_rules;
import 'universal_func.grm' as universal_func;

# It looks as if some punctuation characters within words in Hindi are
# coming about via some portion of another grammar, maybe English,
# that isn't being included here for Bangla. We mimic the behavior
# thus:

buggy_punct = ("¯" | "\"") : " tokens { verbatim: \"\" } ";

bangla_punct_token = Optimize[
 (universal_rules.PUNCT_TOKEN | buggy_punct |
  # NB, using the Devanagari danda for now.
  universal_func.PunctToken["।", universal_rules.token_pause_length_long
               universal_rules.del_space_plus universal_rules.token_phrase_break_true]) <-100>
];

bn_word = Optimize[bn.WORD @ bn.DELETE_JUNK];

# Add in junk words to satisfy Fergus.

other_word = ((universal_rules.kBytes - universal_rules.kSpace) <50000>)+;

word = bn_word | other_word;

# Export for future debugging purposes
export BANGLA_TOKENIZE_AND_CLASSIFY = Optimize[
  bn.BANGLA_NUMERALS_REWRITE @
  universal_func.TokenizeAndClassifySimpler[classify.CLASSIFY,
                               universal_func.WordTokenSimple[word],
                               bangla_punct_token, universal_rules.FALLBACK_TOKEN, universal_rules.SPACE,
                               universal_rules.kAlpha, universal_rules.kDigit]
];

# Filter out the many spurious analysis where one has a diacritic vowel that is
# split off by itself and tagged as a "concise_emoji". While these analyses do
# not typically get the best score, they can still affect search time.  The only
# downside to this approach is that if someone had text with just a diacritic
# vowel, then there would be no output, but then such an input text would be
# illegal anyway and you should get what you pay for.

# Similarly filter out "words" that only consist of a dependent vowel. This was
# causing the spurious generation of ordinals ending in the non-ordinal suffix
# শে which was getting split into শ followed by dependent <e>

bad = Optimize[
    "verbatim: \"" bn.VD "\" type: SEMIOTIC_CLASS concise_emoji: true"
 |  "name: \"" bn.VD+ "\""];

ss = (universal_rules.kBytes | "[junk]")*;

filter_spurious_diacritic_analyses = Optimize[ss - (ss bad ss)];

# This overrides the bad effects of the buggy_punct rule above.

favor_negative_cardinals =
  CDRewrite["cardinal { integer: \"-" universal_rules.kDigit <-150>, "", "", ss]
;

export TOKENIZE_AND_CLASSIFY =
 Optimize[BANGLA_TOKENIZE_AND_CLASSIFY
          @ filter_spurious_diacritic_analyses
          @ favor_negative_cardinals
          @ universal_rules.CLEAN_SPACES]
;
